# Semantic Taxonomy Discovery (Icecat)

**A Thesis-Level Unsupervised Learning Project to recover Product Taxonomies from Raw Text.**

## One-Minute Summary
We applied advanced clustering algorithms (**BIRCH**, **MiniBatchKMeans**, **Ensemble**) to a 1.2GB E-commerce dataset (Icecat) containing **489,898 products** to automatically discover product hierarchies (e.g., *Laptops*, *Tablets*, *Smartphones*) without using labels.

**Key Scientific Finding**:
*   **Our Model (Unsupervised BIRCH)**: Achieved **96.12% Purity** on 500k products.
*   **Scientific Control (Supervised Logistic Regression)**: Achieved **94.27% Accuracy**.
*   **Conclusion**: Our unsupervised approach **exceeds the supervised baseline** by 1.85%, demonstrating that the semantic structure of products is highly discoverable through clustering.

---

## Dataset
The project uses the **Icecat 1.2GB JSON Dataset**.
> **Download Link**: [Google Drive Link](https://drive.google.com/file/d/13f8GHcokLVetrKNM6cFmhaMM0fVCG1NJ/view?usp=sharing)

*   **Size**: 1.2 GB (Raw), 489,898 products.
*   **Input Features**: Title, Brand, Product Description.
*   **Target**: `Category.Name.Value` (Used only for evaluation/testing, never for training).

---

## Installation

1.  **Clone the repository**:
    ```bash
    git clone https://github.com/vivekdhakasd12/Semantic-Taxonomy-Discovery-Icecat
    cd Semantic-Taxonomy-Discovery-Icecat
    ```

2.  **Install Dependencies**:
    ```bash
    pip install -r requirements.txt
    ```

---

## Usage

We optimized the entire pipeline into a single, robust CLI script.

**Run the Full Analysis**:
```bash
python run_analysis.py
```

**What happens?**
1.  **Loads Data**: Processes the full 489,898 product dataset.
2.  **Preprocesses Text**: Cleans HTML tags and applies smart imputation.
3.  **Embeds Text**: Uses `Sentence-BERT` (all-mpnet-base-v2) for 768-dimensional embeddings.
4.  **Reduces Dimensions**: PCA (50 components) for efficiency.
5.  **Tunes Algorithms**: Grid Search on MiniBatchKMeans, BisectingKMeans, and BIRCH.
6.  **Ensemble Clustering**: Combines predictions using voting.
7.  **Supervised Baseline**: Logistic Regression for comparison.
8.  **Cluster Analysis**: Per-cluster quality metrics and error analysis.
9.  **Visualizes**: Generates panels, bar charts, Sankey diagrams, and distributions.

---

## Results and Visualization

All outputs are saved to the `outputs/` directory.

### 1. Performance Metrics (Full Dataset: 489,898 Products)

| Algorithm | Purity | NMI | V-Measure | ARI | Clusters |
| :--- | :--- | :--- | :--- | :--- | :--- |
| **ğŸ† BIRCH** | **96.12%** | 64.93% | 64.93% | 10.82% | 20,805 |
| Supervised Baseline | 94.27% | 92.48% | - | 96.82% | 370 |
| MiniBatchKMeans | 82.24% | 69.89% | 69.89% | 15.08% | 200 |
| Ensemble (Voting) | 82.24% | 69.94% | 69.94% | 15.30% | 200 |
| BisectingKMeans | 75.63% | 67.44% | 67.44% | 18.62% | 150 |

### 2. Cluster Quality Analysis
*   **Median Cluster Size**: 3 products
*   **Mean Cluster Size**: 24 products  
*   **Median Cluster Purity**: 100%
*   **Mean Cluster Purity**: 92.54%

### 3. Visualizations
*   `clustering_comparison_panel.png` - Side-by-side UMAP projections
*   `clustering_metrics_bar.png` - Performance comparison bar chart
*   `cluster_size_distribution.png` - Histogram of cluster sizes
*   `cluster_purity_distribution.png` - Per-cluster purity histogram
*   `sankey_BIRCH.html` - Interactive flow diagram

---

## Recent Updates

### Phase 3: Maximum Output Enhancements (2026-02-08)
*   **Better Embeddings**: Upgraded to `all-mpnet-base-v2` (768 dims).
*   **Extended Metrics**: V-Measure, Fowlkes-Mallows, Homogeneity, Completeness.
*   **Cluster Quality Analysis**: Per-cluster stats, distributions, worst-cluster analysis.
*   **Expanded Tuning**: Broader hyperparameter search.
*   **Ensemble Clustering**: Voting ensemble combining multiple algorithms.
*   **Result**: BIRCH Purity improved from 85.1% to **96.12%** (+11%).

### Phase 2: Full-Scale Training (2026-02-01)
*   Scaled pipeline to full 489,898 products.
*   Added MiniBatchKMeans for scalability.

### Phase 1: Preprocessing Improvements (2026-02-01)
*   HTML Cleaning and Smart Imputation.
*   Data rejection rate: 0.0008%.

---

## Repository Structure
```
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ clustering.py       # Clustering algorithms
â”‚   â”œâ”€â”€ evaluation.py       # Metrics (Purity, NMI, ARI, V-Measure, etc.)
â”‚   â”œâ”€â”€ cluster_analysis.py # Per-cluster quality analysis
â”‚   â”œâ”€â”€ ensemble.py         # Ensemble clustering methods
â”‚   â”œâ”€â”€ features.py         # Text preprocessing & embeddings
â”‚   â””â”€â”€ visualization.py    # Plotting utilities
â”œâ”€â”€ run_analysis.py         # Main entry point
â”œâ”€â”€ outputs/                # Generated reports and images
â””â”€â”€ requirements.txt
```

---

## Acknowledgements

**Project Supervisor**: **Dr. Binh Vu** ([@binhvd](https://github.com/binhvd))

**Author**: Devendra Singh Dhakad  
*Case Study at SRH University of Applied Sciences Heidelberg*


